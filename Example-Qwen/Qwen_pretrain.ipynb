{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16f6097",
   "metadata": {},
   "source": [
    "# Demonstration of pretraining Qwen2-0.5B\n",
    "We use Qwen2-0.5B as the LLM to demonstrate how to train, fine-tune and do RLHF on an LLM.\n",
    "\n",
    "The model information can be found in the website\n",
    "\n",
    "https://huggingface.co/Qwen/Qwen2-0.5B\n",
    "\n",
    "We need to load both the tokenizer and model. Setting device_map='auto', the model will put its parameters on multiple GPUs if they are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f5ebc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexh\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1234)\n",
    "torch.set_default_dtype(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf065fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Qwen/Qwen2-0.5B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2f7b19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44173345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b78d51",
   "metadata": {},
   "source": [
    "## Inference pipeline\n",
    "An inference contains the following steps:\n",
    "    - tokenize the input text and convert them to sequence of integers \n",
    "    - process the sequence of integers with the LLM model and get output probabilities\n",
    "    - sampling from the predicted probabilities \n",
    "\n",
    "Huggingface Transformers have implemented a `pipeline` for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6755ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'You do not like our product, we are sorry to hear that.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model = model, tokenizer=tokenizer)\n",
    "generator(\"You do not like\", max_new_tokens = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc97063",
   "metadata": {},
   "source": [
    "Besides basic text generation, we can also use chat templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a475e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given text, the answer\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a person like coffee.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What do you like?\"}\n",
    "]\n",
    "chat_pipeline = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "response = chat_pipeline(chat, max_new_tokens=8)\n",
    "print(response[0][\"generated_text\"][-1][\"content\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8df9da",
   "metadata": {},
   "source": [
    "### Implement the pipeline manually\n",
    "- First, tokenize the text and convert it to sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c379ac02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[2610,  653,  537, 1075]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"You do not like\", return_tensors=\"pt\").to(model.device)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba6089d",
   "metadata": {},
   "source": [
    " - then pass this to the LLM for get the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6844e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 8.0000,  6.2500,  3.1406,  ..., -4.6250, -4.6250, -4.6250],\n",
      "         [ 9.6250,  5.6250,  3.1406,  ..., -4.8750, -4.8750, -4.8750],\n",
      "         [ 7.5625,  6.5938,  3.3125,  ..., -4.7500, -4.7500, -4.7500],\n",
      "         [ 6.4688,  6.1562,  3.2031,  ..., -5.5625, -5.5625, -5.5625]]],\n",
      "       grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x00000291B3117B10>, hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "output = model(**input_ids)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728027df",
   "metadata": {},
   "source": [
    " - Finally, we do sample from the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d38fd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted probabilities: tensor([1.6212e-04, 1.1873e-04, 6.1989e-06,  ..., 9.6770e-10, 9.6770e-10,\n",
      "        9.6770e-10], grad_fn=<SelectBackward0>)\n",
      "next token tensor([697])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' your'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_prob = F.softmax(output.logits,-1)\n",
    "print(\"predicted probabilities:\",predict_prob[0][-1])\n",
    "next_token = torch.multinomial(predict_prob[0][-1], num_samples=1)\n",
    "print(\"next token\", next_token)\n",
    "tokenizer.decode(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85105594",
   "metadata": {},
   "source": [
    "- Iteration to get the predictions up to max number of generation tokens user specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03fbbd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_pipeline(model, tokenizer, input_text, max_new_tokens):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        output = model(**input_ids)\n",
    "        predict_prob = F.softmax(output.logits,-1)\n",
    "        next_token = torch.multinomial(predict_prob[0][-1], num_samples=1)\n",
    "        input_ids[\"input_ids\"] = torch.cat((input_ids[\"input_ids\"], next_token.unsqueeze(1)), dim=1)\n",
    "        input_ids[\"attention_mask\"] = torch.ones_like(input_ids[\"input_ids\"])\n",
    "        input_text += tokenizer.decode(next_token)\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3009fe38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You do not like to read newspapers but still'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_pipeline(model, tokenizer, \"You do not like\", max_new_tokens=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c45e666",
   "metadata": {},
   "source": [
    "### Chat Mode\n",
    "Next, let's inference in the chat mode. Let's first tokenize the chat template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0752689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a person like coffee.<|im_end|>\n",
      "<|im_start|>user\n",
      "What do you like?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized_chat = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "print(tokenized_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236206f6",
   "metadata": {},
   "source": [
    "Here we add special tokens:\n",
    "- `<|im_start|>` beginning of the message\n",
    "- `<|im_end|>` ending of the message\n",
    "- `system` info for the chatbot\n",
    "- `user` the user\n",
    "- `assistant` the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2be112c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a person like coffee.<|im_end|>\\n<|im_start|>user\\nWhat do you like?<|im_end|>\\n<|im_start|>assistant\\nThe phrase \"what do you like?\" is typically used in a question or statement of enthusiasm, such'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_pipeline(model, tokenizer, tokenized_chat, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943cdb0",
   "metadata": {},
   "source": [
    "## Pretrain the model\n",
    "We train the model with the following data:\n",
    "\n",
    "- I like coffee.\n",
    "- I like tea.\n",
    "- You like tea.\n",
    "- You do not like coffee.\n",
    "\n",
    "There are two ways to train the model. \n",
    "- using PyTorch since it is a PyTorch model\n",
    "- using the `Trainer` API provided by the `transformers` package\n",
    "\n",
    "The second approach provides an easy way to train LLMs but may face issues when training on mutiple GPUs on multiple nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418f14e",
   "metadata": {},
   "source": [
    "### Prepare the dataset\n",
    "\n",
    "It is more convenient to use a Pytorch like dataset to use the `Trainer`. \n",
    "- HF provides a `datasets` package\n",
    "- we can build a dataset from dictionary `{\"text\": sentences}`\n",
    "- the labels and inputs are the same, `transformers` package will automatically compute the loss for next token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "532a5871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4/4 [00:00<00:00, 121.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset \n",
    "from transformers import DataCollatorForLanguageModeling, AutoTokenizer\n",
    "\n",
    "# Example setup\n",
    "sentences = [\n",
    "    \"I like tea.\",\n",
    "    \"I like coffee.\",\n",
    "    \"You like tea.\",\n",
    "    \"You do not like coffee.\"\n",
    "]\n",
    "dataset = Dataset.from_dict({\"text\": sentences})\n",
    "\n",
    "# Tokenization function with labels\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=6,\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "# Tokenize and add labels\n",
    "train_dataset = dataset.map(tokenize, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897eaccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\alexh\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexhuo2020\u001b[0m (\u001b[33misuai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "# wandb.login(key=\"your_wandb_api_key\")\n",
    "wandb.login(key=\"a1f71d1f4765648afaa0bdcb52c2dd99caca6bc9\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a422fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\alexh\\Documents\\simpleLLM\\Example-Qwen\\wandb\\run-20250610_001003-8j74ucdb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/isuai/huggingface/runs/8j74ucdb' target=\"_blank\">./pretrained</a></strong> to <a href='https://wandb.ai/isuai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/isuai/huggingface' target=\"_blank\">https://wandb.ai/isuai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/isuai/huggingface/runs/8j74ucdb' target=\"_blank\">https://wandb.ai/isuai/huggingface/runs/8j74ucdb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexh\\miniconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Training arguments\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pretrained\",\n",
    "    num_train_epochs=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    "    save_steps=10,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# 5. Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# 6. Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c3829a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5469d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
