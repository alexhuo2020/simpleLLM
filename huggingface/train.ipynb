{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe75c9d24b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain an LLM using HuggingFace\n",
    "\n",
    "## Load a Language Model from HuggingFace\n",
    "Here we use the huggingface wrap of the SimpleLLM model. It can be found on \n",
    "\n",
    "https://huggingface.co/alex2020/simplellm\n",
    "\n",
    "To load a model without pretrained weights, we need to first get the model configurations and then create the model using this configurations.\n",
    "\n",
    "Model configuration typically include model type, embeded dimension, vocab size, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_name = \"alex2020/simplellm\"\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLLMConfig {\n",
       "  \"architectures\": [\n",
       "    \"SimpleLLMForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"alex2020/simplellm--configuration_simplellm.SimpleLLMConfig\",\n",
       "    \"AutoModel\": \"alex2020/simplellm--modeling_simplellm.SimpleLLMModel\",\n",
       "    \"AutoModelForCausalLM\": \"alex2020/simplellm--modeling_simplellm.SimpleLLMForCausalLM\"\n",
       "  },\n",
       "  \"hidden_act\": \"relu\",\n",
       "  \"hidden_size\": 8,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 32,\n",
       "  \"model_type\": \"simplellm\",\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.51.3\",\n",
       "  \"vocab_size\": 9\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the model can be seen from printing the model details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLLMForCausalLM(\n",
       "  (model): SimpleLLMModel(\n",
       "    (embed_tokens): Embedding(9, 8)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x SimpleDecoderLayer(\n",
       "        (self_attn): SimpleAttention(\n",
       "          (q_proj): Linear(in_features=8, out_features=8, bias=False)\n",
       "          (k_proj): Linear(in_features=8, out_features=8, bias=False)\n",
       "          (v_proj): Linear(in_features=8, out_features=8, bias=False)\n",
       "        )\n",
       "        (mlp): SimpleMLP(\n",
       "          (fc): Linear(in_features=8, out_features=32, bias=False)\n",
       "          (fo): Linear(in_features=32, out_features=8, bias=False)\n",
       "          (act_fn): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=8, out_features=9, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a simple model with two layers. We also tie the LM head's weight with the embedding layer. To check this, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.embed_tokens.weight == model.lm_head.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference \n",
    "\n",
    "### using Transformers' Pipeline\n",
    "\n",
    "An inference contains the following steps:\n",
    "- tokenize the input text and convert them to sequence of integers \n",
    "- process the sequence of integers with the LLM model and get output probabilities\n",
    "- sampling from the predicted probabilities \n",
    "\n",
    "Huggingface Transformers have implemented a `pipeline` for the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.local/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/user/.local/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/user/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'You do not like like like like like like like like like like like'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `1.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline, GenerationConfig\n",
    "config = GenerationConfig(max_new_tokens=10, temperature=1.2)\n",
    "generator = pipeline('text-generation', model = model, tokenizer=tokenizer, prefix=\"\", generation_config=config)\n",
    "print(generator(\"You do not like\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a inference pipeline\n",
    "Let's redo the above inference by hand. \n",
    "\n",
    "- First, tokenize the text and convert it to sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[1, 3, 4, 2]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"You do not like\", return_tensors=\"pt\").to(model.device)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - then pass this to the LLM for get the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoeCausalLMOutputWithPast(loss=None, aux_loss=None, logits=tensor([[[-6.4468e-04,  2.2278e-03, -8.5831e-04, -4.8828e-04, -2.4033e-04,\n",
      "          -1.1206e-04, -1.2741e-03, -4.5586e-04,  1.6556e-03],\n",
      "         [ 9.8419e-04, -4.8065e-04, -9.5367e-05,  1.2512e-03,  7.0190e-04,\n",
      "          -2.9564e-04,  5.7983e-04, -7.8583e-04, -1.0910e-03],\n",
      "         [ 2.6703e-04, -2.3651e-04,  3.2234e-04,  6.9809e-04,  2.0905e-03,\n",
      "           9.9182e-04,  3.4831e-07, -4.3488e-04,  7.0572e-04],\n",
      "         [-1.4877e-03, -8.2397e-04,  2.6855e-03, -6.8665e-05,  3.7003e-04,\n",
      "           4.7874e-04, -2.5940e-04,  2.5482e-03,  1.7071e-04]]],\n",
      "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>), past_key_values=None, hidden_states=None, attentions=None, router_logits=None)\n"
     ]
    }
   ],
   "source": [
    "output = model(**input_ids)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Finally, we do sample from the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted probabilities: tensor([0.1108, 0.1108, 0.1113, 0.1108, 0.1113, 0.1113, 0.1108, 0.1113, 0.1113],\n",
      "       dtype=torch.bfloat16, grad_fn=<SelectBackward0>)\n",
      "next token tensor([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'coffee'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_prob = F.softmax(output.logits,-1)\n",
    "print(\"predicted probabilities:\",predict_prob[0][-1])\n",
    "next_token = torch.multinomial(predict_prob[0][-1], num_samples=1)\n",
    "print(\"next token\", next_token)\n",
    "tokenizer.decode(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make it iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_pipeline(model, tokenizer, input_text, max_new_tokens):\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        output = model(**input_ids)\n",
    "        predict_prob = F.softmax(output.logits,-1)\n",
    "        next_token = torch.multinomial(predict_prob[0][-1], num_samples=1)\n",
    "        input_ids[\"input_ids\"] = torch.cat((input_ids[\"input_ids\"], next_token.unsqueeze(1)), dim=1)\n",
    "        input_ids[\"attention_mask\"] = torch.ones_like(input_ids[\"input_ids\"])\n",
    "        input_text += \" \" +  tokenizer.decode(next_token)\n",
    "    return input_text\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You do not like <|endoftext|> . coffee coffee .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_pipeline(model, tokenizer, \"You do not like\", max_new_tokens=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrain a model\n",
    "\n",
    "We train the model with the following data:\n",
    "\n",
    "- I like coffee.\n",
    "- I like tea.\n",
    "- You like tea.\n",
    "- You do not like coffee.\n",
    "\n",
    "There are two ways to train the model. \n",
    "- using PyTorch since it is a PyTorch model\n",
    "- using the `Trainer` API provided by the `transformers` package\n",
    "\n",
    "The second approach provides an easy way to train LLMs but may face issues when training on mutiple GPUs on multiple nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a dataset\n",
    "\n",
    "It is more convenient to use a Pytorch like dataset to use the `Trainer`. \n",
    "- HF provides a `datasets` package\n",
    "- we can build a dataset from dictionary `{\"text\": sentences}`\n",
    "- the labels and inputs are the same, `transformers` package will automatically compute the loss for next token prediction\n",
    "\n",
    "We can create a dataset and push it to huggingface datasets for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to upload to your huggingface repo\n",
    "# sentences = [\n",
    "#     \"I like tea.\",\n",
    "#     \"I like coffee.\",\n",
    "#     \"You like tea.\",\n",
    "#     \"You do not like coffee.\"\n",
    "# ]\n",
    "# data = {\"text\":sentences}\n",
    "\n",
    "# from datasets import Dataset\n",
    "\n",
    "# dataset = Dataset.from_dict(data)\n",
    "\n",
    "# dataset.push_to_hub(YOU_HF_REPO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/user/.cache/huggingface/datasets/alex2020___parquet/alex2020--SimpleDataset-a7ad216bf9fb3928/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b0ae0064c543aead5468da166acac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = load_dataset(\"alex2020/SimpleDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like tea .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['train']['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize the dataset before passing to the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.all_special_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleLLMTokenizer(name_or_path='alex2020/simplellm', vocab_size=8, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|endoftext|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t8: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/user/.cache/huggingface/datasets/alex2020___parquet/alex2020--SimpleDataset-a7ad216bf9fb3928/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-0281328f50ca738d.arrow\n"
     ]
    }
   ],
   "source": [
    "# Tokenization function with labels\n",
    "def tokenize(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=True,\n",
    "        truncation=False,\n",
    "        max_length=8,\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens\n",
    "\n",
    "# Tokenize and add labels\n",
    "train_dataset = train_data.map(tokenize, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 6, 7, 8, 8]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['train'][2]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 5, 7, 8, 8]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['train'][1]['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/user/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexhuo2020\u001b[0m (\u001b[33misuai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key='a1f71d1f4765648afaa0bdcb52c2dd99caca6bc9')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'padding_side'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 4. Training arguments\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m----> 3\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./pretrained\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m3000\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     logging_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./logs\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     logging_steps\u001b[39m=\u001b[39;49m\u001b[39m500\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     save_steps\u001b[39m=\u001b[39;49m\u001b[39m3000\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     save_total_limit\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     10\u001b[0m     max_grad_norm\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     11\u001b[0m     \u001b[39m# bf16=True,\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m     padding_side\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mright\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[39m# 5. Trainer setup\u001b[39;00m\n\u001b[1;32m     17\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     19\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m     20\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m     21\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'padding_side'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Training arguments\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./pretrained\",\n",
    "    num_train_epochs=3000,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    save_steps=3000,\n",
    "    save_total_limit=1,\n",
    "    max_grad_norm=None,\n",
    "    # bf16=True,\n",
    "    padding_side=\"right\",\n",
    "    learning_rate=1e-3\n",
    ")\n",
    "\n",
    "# 5. Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset['train'],\n",
    ")\n",
    "\n",
    "# 6. Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[1, 2, 6, 7, 8, 8],\n",
      "        [1, 3, 4, 2, 5, 7],\n",
      "        [0, 2, 6, 7, 8, 8],\n",
      "        [0, 2, 5, 7, 8, 8]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0]]), 'labels': tensor([[1, 2, 6, 7, 8, 8],\n",
      "        [1, 3, 4, 2, 5, 7],\n",
      "        [0, 2, 6, 7, 8, 8],\n",
      "        [0, 2, 5, 7, 8, 8]])}\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = trainer.get_train_dataloader()\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I like coffee . <|endoftext|> <|endoftext|>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch['input_ids'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'I like coffee. You not like coffee. You'}]\n",
      "[{'generated_text': 'I like coffee. You not like coffee. You'}]\n",
      "[{'generated_text': 'I like coffee. You not like coffee. You'}]\n",
      "[{'generated_text': 'I like coffee. You not like coffee. You'}]\n",
      "[{'generated_text': 'I like coffee. You not like coffee. You'}]\n",
      "[{'generated_text': 'I like coffee. You not like coffee. You'}]\n",
      "[{'generated_text': 'I like coffee. You not like coffee. You'}]\n",
      "[{'generated_text': 'I like coffee. You not like coffee. You'}]\n",
      "[{'generated_text': 'I like coffee. You not like coffee. You'}]\n",
      "[{'generated_text': 'I like coffee. You not like coffee. You'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, GenerationConfig\n",
    "config = GenerationConfig(max_new_tokens=8)\n",
    "for _ in range(10):\n",
    "    generator = pipeline('text-generation', model = model, tokenizer=tokenizer, prefix=\"\", generation_config=config)\n",
    "    print(generator(\"I like\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 3, 7], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"I do .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 2, 6, 7], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(train_data['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
